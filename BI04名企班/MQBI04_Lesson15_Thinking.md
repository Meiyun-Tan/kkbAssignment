# Q1:逻辑回归的假设条件是怎样的？

## A1:

逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。

逻辑回归的第一个基本假设是数据服从伯努利分布。在一次伯努利试验中有两个互斥的结果:S和F，S的概率为p，则F的概率为1-p；且n次试验每次都是相互独立的。

$$
f_X(x) = p^x(1-p)^{1-x}
$$

在逻辑回归这个模型里面，假设$$p = h_\theta(x;\theta)$$为样本为正的概率，$$q = 1 - h_\theta(x;\theta)$$为样本为负的概率。

第二个假设：$逻辑回归的概率是线性回归的输出$\theta^{T}x$作为sigmoid函数的输入，经过sigmoid计算所得，也就是$p= \frac{1}{1+e^{-\theta^{T}x}}$

逻辑回归的最终形式:
$$
h_\theta(x;\theta) = \frac{1}{1+e^{-\theta^{T}x}}
$$


# Q2: 逻辑回归的损失函数是怎样的？

## A2:

当*y*=0时，
$$
Cost(h_\theta(x), y) = −log(1−h_\theta(x))
$$
当*y*=1时，
$$
Cost(h_\theta(x), y) = −log(h_\theta(x))
$$


逻辑回归的损失函数是它的极大似然函数:
$$
L\theta = logL(\theta) = -\frac{1}{m}\sum_{i=1}^{m}(y^{i}log(h_{\theta}(x^i)) + (1-y^{i})log(1-h_{\theta}(x^i))
$$
其中：$y^(i)$指样本i的真实label值，而$h_{\theta}(x^i)$指预测为该label的概率；由于概率取对数后为负数，且概率值越高，对数值越高；且我们的目标是希望预测正确的损失函数最小，所以损失函数会在最后使用 - 取反。

# Q3:逻辑回归如何进行分类？

## A3:

逻辑回归的做法是划定一个阈值，正类的概率大于这个阈值的则预测为正类，小于这个阈值则预测为负累。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。

# Q4:为什么在训练中需要将高度相关的特征去掉？

## A4:
+ 高度相关的特征带来的不利影响：
	+ 线性回归估计式变得不确定或不精确；
	+ 线性回归估计式方差变得很大，标准误差增大；
	+ 高度相关的特征太多的话，可能使估计的回归系数符号相反，得出错误的结论；
	+ 而且会削弱特征变量的特征重要性。
+ 因此去掉高度相关的重复特征后：
	+ 会让模型的可解释性更好；
	+ 提高训练的速度。特征多了，会增大训练的时间。