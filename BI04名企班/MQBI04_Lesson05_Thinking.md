# Q1:在CTR点击率预估中，使用GBDT+LR的原理是什么？

## A1:

GBDT+LR算法是具有集成思想的二分类器模型，用来解决二分类问题。

+ GBDT+LR模型原理：
  + GBDT负责对特征进行组合，构造新特征；
    + 使用多棵CART回归树组成，将累加所有树的结果作为最终结果；
    + 当GBDT训练好做预测的时候，把模型中的每棵树计算得到的预测概率值所属的叶子结点位置记为1，其他叶子结点为0，从而构造新的训练数据。
  + LR对GBDT构造的新特征进行训练后，进行分类。

# Q2:Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）?

## A2:

Wide & Deep 模型是 TensorFlow 在 2016 年 6 月左右发布的一类用于分类和回归的模型，并应用到了 Google Play 的应用推荐中。Wide & Deep 模型的核心思想是结合线性模型的记忆能力（memorization）和 DNN 模型的泛化能力（generalization），在训练过程中同时优化 2 个模型的参数，从而达到整体模型的预测能力最优。

记忆（memorization）：从历史数据中发现item或者特征之间的相关性。系统通过获得用户的购物日志数据，包括用户点击哪些商品，购买过哪些商品，然后通过OneHot编码转换为离散特征。Wide模型是线性模型，输入特征可以是连续特征，也可以是稀疏的离散特征。离散特征通过交叉可以组成更高维的离散特征。

泛化（generalization）：基于相关性的传递，发现在历史数据中很少或者没有出现的新的特征组合。Deep模型通过深度学习出一些向量，这些向量是隐性特征，往往没有可解释性的。

通过以下2种方式将wide模型和Deep模型融合：

+ ensemble：两个模型分别对全量数据进行预测，然后根据权重组合最终的预测结果。
+ Joint Training，同时训练Wide模型和Deep模型，并将两个模型的结果的加权作为最终的预测结果，从而让模型具备记忆和泛化能力。

# Q3:在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？

## A3:

+ FM与DNN结合的2种方式：

  1. 并行结构：FM模型和DNN模型分开计算；并将两个模型的结果的加权作为最终的预测结果；
     + 代表模型为：DeepFM，通过FM与DNN的结合，使得高阶特征组合成为可能；

  2. 串行结构：将FM的结果作为DNN的输入；
     +  代表模型：NFM模型，对embedding直接采用对位相乘（element-wise）后相加起来作为交叉特征，然后通过DNN直接将特征压缩，最后concatenate linear部分和deep部分的特征。

# Q4:GBDT和随机森林都是基于树的算法，它们有什么区别？

## A4:

GBDT和随机森林都是集成学习方法中的典型模型，但属于两种不同的集成学习方法。

+ 集成方法分为2种：
  +  **袋装法(bagging)：**
    + 将一个训练集分成多个独立的子训练集分别训练，对多个子模型预测结果进行投票，将投票结果多的作为预测值。
    + 即：通过自助采样的方法生成众多并行式的分类器，通过“少数服从多数的”原则来确定最终的结果。
       + 典型应用就是**随机森林算法**；
            + 优点是每个决策树可以不使用全部特征，可有效避免过拟合。
     + **提升法(boost)：**
          + 将训练集串联起来，一个模型是以另外一个模型的输出为基础进行训练和预测，多个模型的结果进行加权叠加，作为预测值。
          + 即：通过将弱学习器提升为强学习器的集成方法来提高预测精度。
          + 典型应用：Adaboost、**GBDT**。
               + 有3个优点：精度高，且灵活可调；几乎可以不用担心过拟合问题；简化特征工程流程；        

# Q5:item流行度在推荐系统中有怎样的应用?

## A5:

Item的流行程度也称为热度，主要有以下应用：

+ 将榜单中热度的内容推荐给用户，如微博热搜、TopN商品；
+ 解决冷启动问题：根据流行度来推荐商品的算法，也就是什么内容吸引用户，就给用户推荐什么内容；
+ 对于新旧用户根据流行度采用不同的推荐方式：
  + 对于新用户，基于流行度进行推荐
  + 对于老用户，则考虑给高流行度商品进行降权，挖掘长尾商品给老用户；

