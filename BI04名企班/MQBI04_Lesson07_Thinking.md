# Q1：CNN中的参数共享指的是什么？

## A2:

CNN中的参数共享指的是对于每个输出节点，均使用相同的权值矩阵，即同一个卷积核的权值参数相同，所以隐藏层的参数个数和隐藏层的神经元个数无关，只和卷积核的大小和卷积核种类的多少有关。

# Q2：为什么会用到batch normalization ?

## A2:

神经网络的输入值虽然在输入模型前已经进行了无量纲化，但进入网络后，在训练过程中 ，经过激活函数的非线性变换，输入值的分布逐渐发生偏移或者变动。当整体分布逐渐偏往激活函数取值区间的上下限两端时，导致反向传播底层网络的梯度消失，网络收敛变慢。

BN层通过一定的规范化手段，把神经网络任意神经元的输入值的分布重新拉回均值为0、方差为1的标准正态分布，从而使输入值落在激活函数对输入比较敏感的区域，即激活函数梯度变化明显的区域，从而使得梯度变大、避免梯度消失，网络收敛加快。 

# Q3：使用dropout可以解决什么问题？

## A3：

+ Dropout层常用来解决模型过拟合的问题。

Dropout舍弃层核心思想是在层的输出值中引入噪声，减少隐藏层某些节点依赖其他节点才发挥作用的情况。

对某一层使用dropout，就是在训练过程中随机将该层的一些输出特征舍弃（设置为 0）。比如：假设在训练过程中，某一层对给定输入样本的返回值应该是向量 [0.2, 0.5, 1.3, 0.8, 1.1]。但使用 dropout 后，这个向量会按一定dropout 比率将几个随机的元素变成 0，如 [0, 0.5, 1.3, 0, 1.1]。

dropout 比率（dropout rate）是被设为 0 的特征所占的比例，通常在 0.2~0.5范围内。测试时没有单元被舍弃，而该层的输出值需要按 dropout 比率缩小，因为这时比训练时有更多的单元被激活，需要加以平衡。

+ Dropout层可以解决过拟合的原因：

  + 取平均的作用；

    对于没有dropout的标准模型，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时可以采用“5个结果取均值”或“多数取胜的投票策略”去决定最终结果。假如3个网络结果为9，那么很有可能真正的结果是9，其他2个网络给出了错误结果。“综合起来取平均”的策略可以有效防止过拟合问题。不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相取消。

    Dropout掉不同的隐藏神经元就类似于在训练不同的网络，随机删掉一定概率的隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。

  + 减少神经元之间复杂的共适应关系；

    因为dropout会随机断掉某些神经元，从而使两个神经元不一定每次都在一个dropout网络中出现。这些权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效的情况。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。

  + Dropout层类似于性别在生物进化中的角色；

    物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。



