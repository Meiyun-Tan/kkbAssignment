# Q1:新零售中的“人、货、场”分别指的是什么？

## A1:

新零售场景中的永恒的概念，不论技术如何革新，基本要素都离不开“人、货、场”。

+ **人**：**以人为本，构建用户画像（性别、年龄、人生阶段，兴趣爱好），无限逼近消费者内心需求**；

  得益于互联网技术的发展与海量数据的沉淀积累，无限贴近消费者已经成为可能，通过充分利用线上线下的各种数据，让消费者一切行为都能够得到串联，实现由点到面的质变。如此一来，消费者的全息影像便得以生成，商家才能更加有的放矢。

  比如：利用人群分析，对比大促前后增长情况，重点关注增长明星的群体。

+ **货**：**超越成本与价值**。

  在新零售时代，消费者不再满足于商品本身，而是更在意其背后的新内容。企业要尽可能提高与消费者“对话”的频次与质量，这样企业才能知道消费者想要什么。

  数据分析主要包括商品流通环节和零售环节：产品的定价、产品搭配（爆款产品、形象产品、搭配产品），仓储配送、供应链管理，分析产品数据（流量、点击、订单、入篮量）

  比如：通过产品分析了解产品的浏览量、点击量、订单、入篮量、购买用户数等信息，从而帮助企业了解不同商品的用户关注度、购买力等，为产品生命周期分析、产品推广策略提供数据支持。

+ **场**：指消费的场所/场景。**体验与全渠道**。

  当下，消费者身上普遍都打着“SoLoMoPe”（社交化、本地化、移动化、个性化）的标签，他们绝不会满足于一种或是几种消费渠道，而是崇尚在消费的各个阶段都能随时随地购物、娱乐和社交的综合消费体验，并希望无论是通过有形店铺还是无形店铺，甚至是其他媒介渠道，都能够获得一致性的购物体验与营销服务。

  从场景角度说，零售商需要打破线下有形场景与线上无形场景的边界，真正实现零售业态的全渠道升级。

  数据分析主要包括：渠道来源，页面分析，不同店面销售额，城市，商圈，地址。



# Q2:AIPL与传统的品牌资产评估有何区别？

## A2:

在AIPL模型之前，“人群资产”是很难量化，我们只能定性说可口可乐的人群资产比元气森林的多。

AIPL模型（认知Awareness、兴趣Interest、购买Purchase、忠诚Loyalty）是把品牌在电商中的人群资产定量化的运营模型。



# Q3:请列举一例生活工作中存在的帕累托法则

## A3:

帕累托法则以意大利经济学家V.Pareto名字而命名，又称20/80法则，二八定律，帕累托定律。

+ 20/80的法则认为：原因和结果、投入和产出、努力和报酬之间存在着无法解释的不平衡，即：
  + 多数（80%），只能造成少许的影响（20%）
  + 少数（20%），造成主要的、重大的影响（80%）

曾经在阿里的APASS部门，当时这个部门选取的用户，就是基于帕累托法则来定的，即：80%的利润来自于20%的用户，而APASS部门就是专门为这20%的用户提供惊喜服务而成立的。

后来的事实证明，这个决策是正确的，当时正处于天猫国际准备邀请各大品牌入驻的初期，因为这批APASS会员所带来的口碑和平台效应，各奢侈品牌因为一听APASS会员在，都愿意入驻进来。



# # Q4:请简述GBDT与XGBoost的区别？

## A4:

GBDT是一种常用的非线性模型，基于集成学习中的boosting思想，也就是每次迭代都在减少残差的梯度方向新建立一颗决策树，迭代多少次就会生成多少颗决策树。XGBoost是GBDT的工业版本。二者的区别：

+ XGBoost的损失函数，相比GBDT只用到一阶泰勒展开，XGBoost则用到了二阶泰勒展开，因此更加精确。

+ XGBoost将树模型的复杂度加入到正则项中，从而避免过拟合，泛化性能好；

+ XGBoost在寻找最佳分割点时，采用近似贪心算法，用来加速计算；

+ GBDT指的是梯度提升决策树算法。XGBoost不仅支持CART作为基分类器，还支持线性分类器，在使用线性分类器的时候可以使用L1，L2正则化；

+ GBDT没有设计对缺失值的处理，XGBoost能够学习出默认的节点分裂方向来处理缺失值；

+ GBDT每轮使用全部数据，XGBoost支持对数据进行采样。XGBoost将特征列排序后以block的形式存储在内存中，在后面的迭代中重复使用这个结构。在进行节点分裂时，计算每个特征的增益，选择增益最大的特征作为分割节点，各个特征的增益计算可以使用多线程并行。

  

# Q5:如何处理神经网络中的过拟合问题？

## A5:

过拟合是指在验证数据上模型的准确性将达到峰值，**然后停滞或开始下降**。

**处理过拟合的方法如下：**

+ **获得更多的训练数据，**如Data Argumentation（数据增强）：
  使用更多的训练数据是解决过拟合问题最有效的手段，因为更多的样本能够让模型学习到更多更有效的特征，减小噪声的影响。当然，直接增加实验数据一般是很困难的，但是可以通过一定的规则来扩充训练数据。比如：在图像分类的问题上，可以通过图像的平移、旋转、缩放等方式扩充数据；更进一步地，可以使用生成式对抗网络来合成大量的新训练数据。

+ **L1/L2正则化方法：神经网络中损失函数，可以增加一个额外的正则项（L1或L2）正则项看做是损失函数的惩罚项。**

  + L1正则，权值向量w中各个元素的绝对值之和
  + L2正则，权值向量w中各个元素的平方和，然后再求平方根

+ **早停法：**

  其基本含义是在训练中计算模型在验证集上的表现，当模型在验证集上的表现开始下降的时候，停止训练，这样就能避免继续训练导致过拟合的问题。其主要步骤如下：

  1. 将原始的训练数据集划分成训练集和验证集
  2. 只在训练集上进行训练，并每个一个周期计算模型在验证集上的误差，例如，每15次epoch（mini batch训练中的一个周期）
  3. 当模型在验证集上的误差比上一次训练结果差的时候停止训练
  4. 使用上一次迭代结果中的参数作为模型的最终参数

+ **Dropout层：**

  其工作原理可分为以下两个步骤：

  step1： 随机删除处于隐蔽层(Hidden Layer)的神经元，删除比例由人为设定。

  step2：通过重复执行随机删除神经元，形成不同结构的神经网络后，取各个网络结果的均值，达到正则化的效果。

  其优势在于节省时间，同时减少了神经元之间复杂的共适性。因为一个神经元不能依赖其他特定的神经元。因此，不得不去学习随机子集神经元间的鲁棒性的有用连接。对于较复杂的算法和大型的数据来说是一个非常好的选择。

+ **网络结构，降低模型复杂度：**

  在数据较少时，模型过于复杂是产生过拟合的主要因素，适当降低模型复杂度可以避免模型拟合过多的采样噪声。例如，在神经网络模型中减少网络层数、神经元个数等均可以限制网络的拟合能力。

  + Inception结构，加上1x1的卷积核，降低特征图厚度
  + ResBlock结构