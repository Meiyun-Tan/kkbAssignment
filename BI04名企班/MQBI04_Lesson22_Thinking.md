# Q1: 机器学习中的监督学习、非监督学习、强化学习有何区别?

## A1:

**强化学习与监督学习和非监督学习有着本质的区别。**

- **强化学习与监督学习的区别在于**：
  - 对于监督学习，学习者知道每个动作的正确答案是什么，可以通过逐步比对来学习；
  - 对于强化学习，学习者不知道每个动作的正确答案，只能通过奖励信号来学习。强化学习要最大化一段时间内的奖励，需要关注更加长远的性能。
  - 与此同时，监督学习希望能将学习的结果运用到未知的数据，要求结果可推广、可泛化；强化学习的结果却可以用在训练的环境中。
  - 所以，监督学习一般运用于判断、预测等任务，如判断图片的内容、预测股票价格等；而强化学习不适用于这样的任务。
- 强化学习与非监督学习的区别在于：
  - 非监督学习旨在发现数据之间隐含的结构；
  - 而强化学习有着明确的数值目标，即奖励。
  - 它们的研究目的不同。
  - 所以，非监督学习一般用于聚类等任务，而强化学习不适用于这样的任务。



# Q2:什么是策略网络，价值网络，有何区别?

## A2:

策略网络和价值网络是强化学习两种重要的方法。

- **策略网络**的实质是建立一个神经网络模型，通过观察环境状态预测出目前应该执行的策略（动作1，状态1）、（动作2，状态2），执行这个策略，并获取可以获得最大的奖励是多少。和普通的监督学习不同，策略网络不是通过feature预测label，而是根据对观察的环境状态进入模型，得到action，执行这个action后得到reward，通过reward的加权衰减并叠加后计算梯度，通过梯度优化网络参数。

- **价值网络**通过计算目前状态s的累积分数的期望，价值网络给游戏中的状态赋予一个分数（数值），每个状态都经历了整个数值网络；
  	奖励更多的状态，会在数值网络中的数值Value更大；
  	这里的奖励是奖励期望值，我们会从状态集合中选择最优的。

- **二者的区别在于：**
  - 策略网络的输出，是一个落子的概率分布
  - 价值网络的输出，一个可能获胜的数值，即“价值”；
    - 这个价值训练是一种回归( regression)，即调整网络的权重来逼近每一种棋局真实的输嬴预测；
    - 对于价值网络，当前局面的价值 = 对终局的估计
  - 策略网络能学到一些随机策略，而价值网络通常是学不到随机策略



# Q3: 请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的?

## A3:

蒙特卡洛算法，不是一种算法的名称，而是对一类随机算法的特性的概括。

**思想：**是一种随用大量随机采样逼近最优解的方法。

**原理**：采样越多，越近似最优解

MCTS，蒙特卡洛树搜索，结合了随机模拟的一般性和树搜索的准确性。
作用是通过模拟来进行结果预测，理论上可以用于以{ state, action}为定义的任何领城；

- **步骤**：
  - step1：选择（Select），从根节点开始，按一定策略，搜索到叶子节点；
  - step2：扩展（Expansion），对叶子节点扩展一个或多个合法的子节点
  - step3：模拟（Simluation），对子节点采用随机的方式(这也是为什么称之为蒙特卡洛的原因)模拟若干次实验。模拟到最终状态时即可得到当前模拟所得的分数；
  - step4：回传（Backpropagation），根据子节点若干次模拟的得分，更新当前子节点的模拟次数与得分值。同时将模拟次数与得分值回传到其所有祖先节点并更新祖先节点。




# Q4: 假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑?

## A4:

使用强化学习与自适应在线学习，通过持续机器学习和模型优化建立决策引擎，对海量用户行为以及item特征进行实时分析，帮用户快速发现喜欢的短视频。提高人与短视频之间的匹配效率。

考虑用户是否点击视频、是否收藏视频、是否转发视频等行为，将这些行为作为奖励，反馈给强化学习的模型，通过强化学习学习出用户的喜好。





# Q5: 在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路

## A5: 

自动驾驶场景中：

- 强化学习的智能体（Agent）为车

- 可以通过摄像头、雷达、激光测距仪、传感器等观测环境，获取丰富的环境信息

- 动作（Action）包括加速、减速、转向；

- 状态，State，Agent从环境获取的信息

- 奖励（Reward）：每次执行动作后，到达目的地如果路程更短，则执行对动作的奖励（正值）；若安全抵达就给更多奖励（正值）；反之则为惩罚（负值）；

- 每一步行动，环境都给与反馈，通过反馈不断调整训练对象的行为。

  





