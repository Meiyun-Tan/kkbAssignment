# Q1: 请简述基于蒙特卡洛的强化学习的原理？

## A1:

强化学习考虑的是个体（Agent）与环境（Environment）的交互问题。它的目标是找到一个最优策略，使个体（Agent）获得尽可能多的来自环境（Environment）的奖励。

蒙特卡罗强化学习（Monte-Carlo Reinforcement Learning，简称MC强化学习）：个体在不清楚MDP状态转移概率的情况下，直接从所经历过的完整状态序列（Episode）来估计状态的真实价值，并认为某状态的价值等于在多个状态序列中状态所有收获值的平均值。

完整的状态序列（Complete Episode）是指从某一个状态开始，个体与环境交互，直到环境给出终止状态的奖励为止。完整的状态序列不要求起始状态一定是某一个特定的状态，但是要求个体最终要进入环境认可的某一个终止状态。
蒙特卡罗强化学习有如下特点：不依赖状态转移概率，直接从所经历过的完整状态序列中学习，就是用平均收获值代替价值。理论上完整的状态序列越多，结果越准确。



# Q2:强化学习的目标与深度学习的目标有何区别？

## A2: 

**强化学习**属于机器学习的分支（其他还有监督学习、无监督学习）。与其他学习方法不同之处在于：强化学习是智能体从环境到行为映射的学习，**目标是让奖励最大化**。

强化学习的目标可以通过深度学习实现。

而深度学习的目标除了强化学习的目标，还包括监督学习的目标、无监督学习的目标等。

